{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('json_for_Muhammad.json')\n",
    "data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\4a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\4a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "from nltk import Tree\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we are using 2 different POS taggers, 1 is of flair and the other one is of NLTK. The flair POS is more accurate\n",
    "# but takes time to process while nltk is quick but can give less accurate results. Mainly we are using NLTK but\n",
    "# at some point nltk was not giving good results so we used flair at that spot\n",
    "\n",
    "# this takes a sentence and returns its POS tags using nltk\n",
    "def preprocess1(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main processing is happing here. The parameter is the data the starting recipe and the ending recipe\n",
    "def process(data,starting_recipe=0,ending_recipe=2):\n",
    "# creating a empty variable to save final output\n",
    "    df = None\n",
    "# we have to manage 3 things from it now the food its quantity and its unit. So we creating 3 list to store \n",
    "# all of them in parallel\n",
    "    des,quantity,unit = [],[],[]\n",
    "    \n",
    "# these are the words that we don't want in the food names. you can thing of them as stop words that we want to \n",
    "# remove and have no importance\n",
    "    unknow_words = ['chopped',\"slices\",\"pinch\",\"large\",\"medium\",\"small\",\"each\",\"cooked\",\"uncooked\",\"cooled\",\"pot\", \"pouch\", \"packed\" ,\"package\", \"regular\", \"roughly\" ,\"slice\", \"sliced\", \"strips\"]\n",
    "# all the units that we can have in our recipes\n",
    "    units = [' ml ', ' cup ',' can ',' cans ', ' cups ',\" liter \",\" liters \", \" cm \",\" in \",' g ', ' oz ',' lb ', ' kg ',' kgs ', ' cm ', ' tbsp ', \" ounces \",\" lbs \",' tsp ', ' l ',\" ea \"]\n",
    "\n",
    "# in case we are left with some units which our algo didn't successfully remove we can at the end remove using this\n",
    "# list\n",
    "    units_remove = ['%','/','.','-','_',']','*','-/', 'c.', 'à', 'thé','hot','pkg','half','and','cubed','cube', '/-inch', \"in\",\"cm\",',packages', 'pitted', 'plain' ,'thin' ,'-inch' ,'inch' ,'cube', 'cubes','thinly','tiny','finely','[','ml', 'cup',\"cups\",'can','cans', \"liter\",\"-in\",\"liters\", 'g', 'oz', 'kg', 'cm','lb','-lb' ,'tbsp', 'kgs','tsp', 'l',\"ea\",\"ounces\",\"lbs\",\"peeled\",\"roasted\"]\n",
    "\n",
    "#     we will now loop over all the data we have. we are using data[\"data\"] as its the format of how our data is \n",
    "# stored in json\n",
    "    for result in data['data'][starting_recipe:ending_recipe]:\n",
    "    #     print(result[\"title\"])\n",
    "    \n",
    "#     after excessing every recipe we need to extract its ingredients and loop over it to process them one by one\n",
    "# in d we are storing in a form of list all the ingredients required for 1 recipe\n",
    "        d= result[u'ingredients'][u'ungrouped'][u'list']\n",
    "    \n",
    "# we will loop over the ingredients now\n",
    "        for a in d:\n",
    "#         extracting the description of ingredient\n",
    "            m= a['description']\n",
    "#     removing all the dots inside the description\n",
    "            m = re.sub(\"[.]\",\" \",m)\n",
    "#     tokenizing and then rejoining. this is done to have space before and after every word so we can better process\n",
    "            word_tokens = word_tokenize(m) \n",
    "            m  = (\" \").join(word_tokens)\n",
    "            \n",
    "# We will split the description based on the ,\n",
    "# we only need the part before , but the pattern breaks at only 2 places \n",
    "#     1- when we have NN,NN and when we have VBD,VBD eg boneless,chicken and cooled,cooked farro\n",
    "            split_string1 = m.split(',')\n",
    "\n",
    "# we are using POS tags and regexp together to catch any required pattern and if the pattern is detected keep \n",
    "# both sides of split\n",
    "\n",
    "# defining the patterns\n",
    "            done = 0\n",
    "            if \",\" in m:\n",
    "                patterns1 = \"\"\"P: {<NN><,><NN>}\n",
    "                                {<VBD><,><VBD>}\n",
    "                                {<JJ><,><JJ>}    \n",
    "                                {<VBN><,><VBN>}\"\"\"  # last one new change if issues due to 4 small roasted , peeled fresh beets , julienned\n",
    "\n",
    "#                 2 small green , red and/or yellow bell peppers , thinly sliced\n",
    "                \n",
    "                #     giving regexp the patterns\n",
    "                PChunker1 = RegexpParser(patterns1)\n",
    "#     preprocess1() applied to function to get POS tags and then given to regexp to check it.\n",
    "                output_pattern1 = PChunker1.parse(preprocess1(m))\n",
    "#         the loop is check whether the pattern was found or not if found we will keep both side of split\n",
    "                for child in output_pattern1:\n",
    "                    if isinstance(child, Tree):              \n",
    "                        if child.label() == 'P':\n",
    "\n",
    "                            if (child[0][1] == \"VBD\") and len(split_string1)<3:\n",
    "\n",
    "                                done=1\n",
    "                                substring2 = split_string1[0] + \",\" +split_string1[1]\n",
    "                            elif child[0][1] == \"NN\" or child[0][1] == \"VBN\" or child[0][1] == \"JJ\":\n",
    "                                done=1\n",
    "                                substring2 = split_string1[0] + \",\" +split_string1[1]\n",
    "\n",
    "# if no pattern was found then we only need the first side and we do that here\n",
    "            if done == 0:\n",
    "                substring2 = split_string1[0]         \n",
    "# spliting based on or. we don't need both side of or we only need the first side of or\n",
    "# the pattern breaks only on JJ or JJ where we have to take the 2nd side of or eg dark or light break\n",
    "            substring2 = re.sub(\"and/or\",\"or\",substring2)\n",
    "            substring2 = re.sub(\"or/and\",\"or\",substring2)\n",
    "            split_string = substring2.split(' or ')\n",
    "            substring1 = \"\"\n",
    "# we are doing same again making pattern giving it to regexp, getting pos for sentences and then checking if it\n",
    "# exists or not\n",
    "            done = 0\n",
    "#     \n",
    "            patterns = \"\"\"P: {<JJ><CC><JJ>}\n",
    "                             {<VBD><CC><VB>}\n",
    "                             {<NN><CC><JJ>}\n",
    "                             {<VBD><CC><JJ>}\n",
    "                             {<CD><CC><CD>}\"\"\" #9-inch unbaked or frozen deep-dish pie crust *\n",
    "            PChunker = RegexpParser(patterns)\n",
    "            if \" or \" in substring2:\n",
    "                found_pat = PChunker.parse(preprocess1(substring2))\n",
    "#             else: \n",
    "#                 found_pat = PChunker.parse(preprocess1(substring2))\n",
    "\n",
    "                for child in found_pat:\n",
    "                    if isinstance(child, Tree):               \n",
    "                        if child.label() == 'P':\n",
    "                            if child[1][0] == \"or\":\n",
    "                                done=1\n",
    "                                substring1 = split_string[1]\n",
    "\n",
    "            if done ==0:\n",
    "                substring1 = split_string[0]\n",
    "\n",
    "# we are lowering the whole string. thats due to the reason that we where having units in mixed cases like\n",
    "# mL ml, ML and else so we need to lower it down all\n",
    "            substring1 = substring1.lower()\n",
    "# here we are spliting based on the units. we are giving priority to the unit which is coming after in description\n",
    "# eg 1 g (20 ML) water so here ML will be prefered unit. We are storing that unit as well for later use\n",
    "# after finding the unit we have to remove every thing before it as it was a pattern in all the descriptions\n",
    "            index = 0\n",
    "            u = \"x\"\n",
    "            for x in units:\n",
    "                finding = max(substring1.find(x),0)\n",
    "                if finding > index:\n",
    "                  index =finding\n",
    "                  u = x\n",
    "\n",
    "            if index !=0:\n",
    "                index = index + len(u.strip())+1\n",
    "\n",
    "#            this portion remove words inside bracket eg (optional)\n",
    "#             substring3 = substring1[index:]\n",
    "            substring3 = substring1\n",
    "            substring3 = re.sub(\"\\(.*?\\)\",\"\",substring3)\n",
    "            \n",
    "# this portion clean the string by removing the brackets and its corresponding text\n",
    "            index = [i for i in range(len(substring3)) if substring3.startswith(\"(\", i)]\n",
    "\n",
    "            if len(index) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                index = index[0]\n",
    "                substring3 = substring3[:index]  \n",
    "\n",
    "\n",
    "            index = [i for i in range(len(substring3)) if substring3.startswith(\")\", i)]\n",
    "\n",
    "            if len(index) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                index = index[-1]\n",
    "                substring3 = substring3[index+1:]      \n",
    "# in the final name we don't have any kind of numbers. This portion is to detect any remaining no inside the\n",
    "# description and remove them \n",
    "            x = re.findall(\"\\d\", substring3)\n",
    "\n",
    "#             try:\n",
    "#                 substring4 = substring3.split(x[-1])\n",
    "#                 substring4 = substring4[-1]\n",
    "#             except:\n",
    "#                 substring4 = substring3\n",
    "            for aaa in set(x):\n",
    "                substring3 = re.sub(aaa,\"\",substring3)\n",
    "            substring4 = substring3\n",
    "# removing the stop words which we have defined above\n",
    "            word_tokens = word_tokenize(substring4) \n",
    "            filtered_sentence = [w for w in word_tokens if not w in unknow_words+units_remove] \n",
    "            substring4  = (\" \").join(filtered_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# removing extra spaces percentage signs and commas which are coming in starting or ending of strings\n",
    "            substring4 = substring4.strip(\",\")\n",
    "            substring4 = substring4.strip()\n",
    "            \n",
    "            if  \"boneless\" ==  substring4:\n",
    "                print(m)\n",
    "                print(substring1)\n",
    "                print(substring2)\n",
    "                print(substring3)\n",
    "                print(substring4)\n",
    "#             print(substring4)\n",
    "\n",
    "\n",
    "#     adding data to all three lists and creating a dataframe out of it\n",
    "            des.append(substring4)\n",
    "            quantity.append(a['quantity'])\n",
    "            unit.append(u)\n",
    "            df =pd.DataFrame(zip(des,quantity,unit),columns=[\"food\",\"quantity\",\"unit\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function with data starting and ending index\n",
    "start = 2\n",
    "end = 23\n",
    "start = max(0,start)\n",
    "end = min(end,len(data))\n",
    "df = process(data,0,455)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>knorr â® zaâ€™atar seasoning blend</td>\n",
       "      <td>60.00</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>rapeseed oil</td>\n",
       "      <td>0.25</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>head cauliflower</td>\n",
       "      <td>1.00</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>hellmann'sâ® real mayonnaise</td>\n",
       "      <td>60.00</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>eggplant</td>\n",
       "      <td>1.00</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 food  quantity   unit\n",
       "0  knorr â® zaâ€™atar seasoning blend     60.00    ml \n",
       "1                        rapeseed oil      0.25   cup \n",
       "2                    head cauliflower      1.00      x\n",
       "3        hellmann'sâ® real mayonnaise     60.00    ml \n",
       "4                            eggplant      1.00      x"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "df1 = df['food'].apply(lambda w: Word(w).singularize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this portion we read all our categories file and store the data in form of list\n",
    "bakery = []\n",
    "with open(\"Bakery.txt\") as f:\n",
    "    bakery = [re.sub(\"\\n\",\"\",a).strip() for a in f.readlines()]\n",
    "\n",
    "dairy = []\n",
    "with open(\"Dairy & Eggs.txt\") as f:\n",
    "     dairy = [re.sub(\"\\n\",\"\",a).strip() for a in f.readlines()]\n",
    "\n",
    "drinks = []\n",
    "with open(\"Drinks.txt\") as f:\n",
    "    drinks = [re.sub(\"\\n\",\"\",a).strip() for a in f.readlines()]\n",
    "fruit = []\n",
    "with open(\"Fruit & Vegetables.txt\") as f:\n",
    "    fruit = [re.sub(\"\\n\",\"\",a).strip() for a in f.readlines()]\n",
    "\n",
    "meat = []\n",
    "with open(\"Meat & Fish.txt\") as f:\n",
    "    meat = [re.sub(\"\\n\",\"\",a).strip() for a in f.readlines()]\n",
    "\n",
    "pantry = []\n",
    "with open(\"Pentry.txt\") as f:\n",
    "    pantry = [re.sub(\"\\n\",\"\",a).strip() for a in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which will determine which ingredient is of which category. It will match one by one with the list and\n",
    "# as it will find a results it will return that category\n",
    "def name_cat(name):\n",
    "  check_bakery,check_pantry,check_meat,check_fruit,check_drink,check_dairy = False,False,False,False,False,False\n",
    "  for a in fruit:\n",
    "    if a in name and len(a)>0:\n",
    "      return 0\n",
    "\n",
    "  for a in dairy:\n",
    "    if a in name and len(a)>0:\n",
    "      return 1\n",
    "  \n",
    "  for a in bakery:\n",
    "    if a in name and len(a)>0:\n",
    "      return 2\n",
    "  \n",
    "  for a in drinks:\n",
    "    if a in name and len(a)>0:\n",
    "      return 3\n",
    "  \n",
    "  for a in meat:\n",
    "    if a in name and len(a)>0:\n",
    "      return 4\n",
    "  \n",
    "  for a in pantry:\n",
    "    if a in name and len(a)>0:\n",
    "      return 5\n",
    "\n",
    "  return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the total based on the food name and its unit\n",
    "df1 = df.groupby([\"food\",\"unit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping our the grouped dataframe to extract the required info and get its category check and then store that \n",
    "# to a list\n",
    "json_pantry = []\n",
    "json_dairy = []\n",
    "json_meat = []\n",
    "json_bakery = []\n",
    "json_vegetable = []\n",
    "json_drink = []\n",
    "json_others = []\n",
    "for name,group in df1:\n",
    "    new = {}\n",
    "    new[\"ingredient\"]=name[0]\n",
    "    new[\"unit\"]=name[1]\n",
    "    new[\"quantity\"]=group[\"quantity\"].sum()\n",
    "    cat = name_cat(name[0]) \n",
    "    if cat == 0:\n",
    "        json_vegetable.append(new)\n",
    "    elif cat == 1:\n",
    "        json_dairy.append(new)\n",
    "    elif cat == 2:\n",
    "        json_bakery.append(new)\n",
    "    elif cat == 3:\n",
    "        json_drink.append(new)\n",
    "    elif cat == 4:\n",
    "        json_meat.append(new)\n",
    "    elif cat == 5:\n",
    "        json_pantry.append(new)\n",
    "    elif cat == 6:\n",
    "        json_others.append(new)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>knorr â® zaâ€™atar seasoning blend</td>\n",
       "      <td>60.00</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>rapeseed oil</td>\n",
       "      <td>0.25</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>head cauliflower</td>\n",
       "      <td>1.00</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>hellmann'sâ® real mayonnaise</td>\n",
       "      <td>60.00</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>eggplant</td>\n",
       "      <td>1.00</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>fresh pumpkin</td>\n",
       "      <td>225.00</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>halved cherry tomatoes</td>\n",
       "      <td>125.00</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>sour dough bread</td>\n",
       "      <td>4.00</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>baby arugula</td>\n",
       "      <td>375.00</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>toasted pumpkin seeds</td>\n",
       "      <td>45.00</td>\n",
       "      <td>ml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   food  quantity   unit\n",
       "0    knorr â® zaâ€™atar seasoning blend     60.00    ml \n",
       "1                          rapeseed oil      0.25   cup \n",
       "2                      head cauliflower      1.00      x\n",
       "3          hellmann'sâ® real mayonnaise     60.00    ml \n",
       "4                              eggplant      1.00      x\n",
       "..                                  ...       ...    ...\n",
       "449                       fresh pumpkin    225.00     g \n",
       "451              halved cherry tomatoes    125.00    ml \n",
       "452                    sour dough bread      4.00      x\n",
       "453                        baby arugula    375.00    ml \n",
       "454               toasted pumpkin seeds     45.00    ml \n",
       "\n",
       "[392 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
